{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When creating predictive models, analysts often find themselves working with imbalanced datasets. These datasets, which are marked by the disproportionate distribution of observations across categories associated with a given field, are quite common, appearing in industries such as fraud detection, disease screening, and more. \n",
    "\n",
    "What is particularly frustrating about imbalanced datasets is the fact that they render conventional accuracy measurements, at best, unhelpful, and at worst, virtually useless. This is because classification models trained on such datasets tend to (rather intelligently) predict that nearly all observations belong to the category overrepresented in the data. Put differently, a model trained on a dataset with 100 observations, 95 of which belong to category “A”, will likely achieve an accuracy of around 95%, predicting nearly all observations belong to category “A”. Although a model with such a high accuracy score might seem useful, it would prove untenable if the 5 observations in category “B” were the fraudsters or diseases analysts had set out to identify. \n",
    "\n",
    "In this post, I will tackle the problem of class imbalance and discuss how to use common resampling techniques to create models which better classify underrepresented categories. The analysis will utilize one quarter of mortgage acquisition and performance data from Fannie Mae."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When and Why “Accuracy” Doesn’t Cut It"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate the ineffectiveness of accuracy measurements when dealing with imbalanced datasets, let’s attempt to create a model to predict whether or not a mortgage will receive a modification at any point during its lifetime. Our first step in this process is to import both our dependencies and the datasets we will be using. To both speed up and simplify our analysis, let's use just one quarter of Fannie Mae data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################\n",
    "# Step 1) Import Dependencies & Data #\n",
    "######################################\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#set root directory\n",
    "root_directory = \"/Users/scottinderbitzen/Desktop/Fannie Data\"\n",
    "\n",
    "#acquisition data\n",
    "acquisition_data_path = root_directory + \"/Acquisition_2006Q1.txt\"\n",
    "df_acquisition = pd.read_table(acquisition_data_path, delimiter = \"|\",\n",
    "    names = ['LOAN_IDENTIFIER', 'CHANNEL', 'SELLER_NAME', 'ORIGINAL_INTEREST_RATE', \n",
    "             'ORIGINAL_UNPAID_PRINCIPAL_BALANCE_(UPB)', 'ORIGINAL_LOAN_TERM', \n",
    "             'ORIGINATION_DATE ', 'FIRST_PAYMENT_DATE', 'ORIGINAL_LOAN-TO-VALUE_(LTV)', \n",
    "             'ORIGINAL_COMBINED_LOAN-TO-VALUE_(CLTV)', 'NUMBER_OF_BORROWERS', \n",
    "             'DEBT-TO-INCOME_RATIO_(DTI)', 'BORROWER_CREDIT_SCORE', \n",
    "             'FIRST-TIME_HOME_BUYER_INDICATOR','LOAN_PURPOSE', 'PROPERTY_TYPE',\n",
    "             'NUMBER_OF_UNITS', 'OCCUPANCY_STATUS', 'PROPERTY_STATE', 'ZIP_(3-DIGIT)', \n",
    "             'MORTGAGE_INSURANCE_PERCENTAGE', 'PRODUCT_TYPE', 'CO-BORROWER_CREDIT_SCORE', \n",
    "             'MORTGAGE_INSURANCE_TYPE', 'RELOCATION_MORTGAGE_INDICATOR'])\n",
    "\n",
    "\n",
    "#performance data\n",
    "performance_data_path = root_directory + \"/Performance_2006Q1.txt\"\n",
    "df_performance = pd.read_table(performance_data_path, delimiter = \"|\",\n",
    "    names = ['Loan_Identifier', 'Monthly_Reporting_Period', 'Servicer_Name', \n",
    "    'Current_Interest_Rate', 'Current_Actual_UPB', 'Loan_Age', 'Remaining_Months_Legal_Matrty', \n",
    "    'Adjusted_Months_Remaining_Matrty', 'Matrty_Date',\n",
    "    'Metropolitan_Statistical_Area', 'Current_Loan_Delq_Status', 'Modification_Flag', \n",
    "    'Zero_Balance_Code', 'Zero_Balance_Effective_Flag', 'Last_Paid_Installment_date', \n",
    "    'Foreclosure_Date', 'Disposition_Date', 'Foreclosure_Costs', 'Property_Pres_Ren_Costs', \n",
    "    'Asset_Recovery_Costs', 'Misc_Holding_Expenses_Credits', 'Associated_Taxes_Holding', \n",
    "    'Net_Sale_Proceeds', 'Credit_Enhancements_Proceeds', 'Repurchase_Make_Whole_Proceeds', \n",
    "    'Other_Foreclosure_Proceeds', 'Non_Interest_Bearing_UPB', 'Principal_Forgiveness_UPB', \n",
    "    'Repurchase_Make_Whole_Flag', 'Foreclosure_Principal_Write_Off_Amount', \n",
    "    'Servicer_Activity_Indicator'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have imported our data and stored them in pandas dataframes, our next step is to clean the dataframes and prepare them for analysis. To do this, we will first determine which loans in the \"df_performance\" dataframe have recevied modifications by isolating unique loan identifiers where the \"Modification_Flag\" is equal to \"Y\". Then, we will add a field to our \"df_acquisition\" dataframe called \"MODIFIED\". We will assign a value of \"Yes\" to our newly created \"MODIFIED\" field for each observation with a unique loan identifier isolated in the previous step. We will assign a value of \"No\" to this field for all other observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Step 2) Clean Data & Prepare for Analysis #\n",
    "#############################################\n",
    "\n",
    "#Find all unique loan ids that have a modification flag with a value of \"Y\"\n",
    "modified_loans = df_performance.loc[df_performance[\"Modification_Flag\"]==\"Y\", \"Loan_Identifier\"].unique()\n",
    "\n",
    "#Add modification flag to acquisition dataset\n",
    "df_acquisition[\"MODIFIED\"] = np.where(df_acquisition[\"LOAN_IDENTIFIER\"].isin(modified_loans),\"Yes\",\"No\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our \"MODIFIED\" field has been created, we will next convert it into a dummy variable where a \"1\" indicates a loan has received a modification and a \"0\" indicates a loan has not been modified. Then, we will create a dummy variable for \"Mortgage_Insurace_Type\". Finally, we will clean observations with \"NaN\" values, dropping them or converting them to zero where sensible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convert modification flag to dummy variable\n",
    "df_acq_mod_dummies = pd.concat([df_acquisition, pd.get_dummies(df_acquisition[\"MODIFIED\"],drop_first = True)],\n",
    "                                axis=1); df_acquisition\n",
    "\n",
    "#Create mortgage insurance type dummy\n",
    "df_acq_mod_dummies[\"MORTGAGE_INSURANCE_TYPE_1\"] = np.where(df_acq_mod_dummies[\"MORTGAGE_INSURANCE_TYPE\"]==1,1,0)\n",
    "df_acq_mod_dummies[\"MORTGAGE_INSURANCE_TYPE_2\"] = np.where(df_acq_mod_dummies[\"MORTGAGE_INSURANCE_TYPE\"]==2,1,0)\n",
    "df_acq_mod_dummies = df_acq_mod_dummies.drop(\"MORTGAGE_INSURANCE_TYPE\",1)\n",
    "\n",
    "#Make mortgage insurance percentage and dti 0 for nan\n",
    "df_acq_mod_dummies[\"MORTGAGE_INSURANCE_PERCENTAGE\"].fillna(0, inplace=True)\n",
    "df_acq_mod_dummies[\"DEBT-TO-INCOME_RATIO_(DTI)\"].fillna(0, inplace=True)\n",
    "\n",
    "#Drop reamining N/A\n",
    "df_acq_mod_dummies = df_acq_mod_dummies.dropna(axis=0, how=\"any\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step in preparing our data for analysis is choosing which features we want to include in our model. To keep things simple, let's include all of the fields but the unique \"Loan_Identifier\". Lastly, we will split our data into training and testing sets. Our training sets will include 67% of our data and we will hold out the reamining 33% for use in model testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create dataframe of independent variables, dropping loan ID and \"Yes\" (modification dummy)\n",
    "features = ['CHANNEL', 'SELLER_NAME', 'ORIGINAL_INTEREST_RATE', \n",
    "            'ORIGINAL_UNPAID_PRINCIPAL_BALANCE_(UPB)', 'ORIGINAL_LOAN_TERM', \n",
    "            'ORIGINATION_DATE ', 'FIRST_PAYMENT_DATE', 'ORIGINAL_LOAN-TO-VALUE_(LTV)', \n",
    "            'ORIGINAL_COMBINED_LOAN-TO-VALUE_(CLTV)', 'NUMBER_OF_BORROWERS', \n",
    "            'DEBT-TO-INCOME_RATIO_(DTI)', 'BORROWER_CREDIT_SCORE', \n",
    "            'FIRST-TIME_HOME_BUYER_INDICATOR','LOAN_PURPOSE', 'PROPERTY_TYPE','NUMBER_OF_UNITS', \n",
    "            'OCCUPANCY_STATUS', 'PROPERTY_STATE', 'ZIP_(3-DIGIT)', \n",
    "            'MORTGAGE_INSURANCE_PERCENTAGE', 'PRODUCT_TYPE','RELOCATION_MORTGAGE_INDICATOR', \n",
    "            \"MORTGAGE_INSURANCE_TYPE_1\", \"MORTGAGE_INSURANCE_TYPE_2\"]\n",
    "x = pd.get_dummies(df_acq_mod_dummies[features], drop_first = True)\n",
    "y = df_acq_mod_dummies[\"Yes\"]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.options.mode.chained_assignment = None\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data are now clean and ready to be used in model training. To demonstrate the misleading nature of the accuracy measurement when used with imbalanced datasets, let’s train a simple k-nearest neighbors (KNN) model on our training set and see how it performs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model achieves an accuracy score of:\n",
      "95.03218438538205 %\n"
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "# Step 3) Create basic K-Nearest Neighbors Model #\n",
    "##################################################\n",
    "\n",
    "#Test accuracy of a k-nearest neighbors classifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#set nearest neighbors classifier\n",
    "knn.fit(x_train, y_train)\n",
    "y_predict_knn = knn.predict(x_test)\n",
    "\n",
    "#print the accuracy of the KNN model\n",
    "print(\"The model achieves an accuracy score of:\")\n",
    "print(accuracy_score(y_test, y_predict_knn)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even without any hyperparameter tuning, the KNN model achieves an accuracy of 95.03%. In some cases, this would be good enough that an analyst might call it a day and head home. In this case however, our model may not actually be doing what we want it to -- there is more work to be done. Let's examine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model predicted 195 modifications out of a total 38528\n",
      "predictions, represting < 1.0 %.\n",
      "This stems from the fact that modifications make up only\n",
      "4.64 % of the dataset.\n"
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "# Step 4) Examine Accuracy Against Other Metrics #\n",
    "##################################################\n",
    "\n",
    "print(\"The model predicted\", y_predict_knn.sum(), \"modifications out of a total\", y_predict_knn.size, )\n",
    "print(\"predictions, represting <\", np.round(y_predict_knn.sum()/y_predict_knn.size,2)*100,\"%.\")\n",
    "print(\"This stems from the fact that modifications make up only\")\n",
    "print(np.round((df_acq_mod_dummies.Yes.sum()/df_acq_mod_dummies.Yes.count())*100,2),\"% of the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though our accuracy score is quite high, we can see that our model is predicting a negligible number of modifications. In other words, if we simply predicted no loans would receive modifications, we would achieve a very similar accuracy score. Given we want our model to be able to isolate observations indicative of modifications, our current KNN model will not suffice. Let's use the area under a ROC curve (AUROC) to further demonstrate this point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.56\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(np.round(roc_auc_score(y_test,y_predict_knn)*100,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our AUROC score is about 51%. While a full description of ROC curves falls outside the scope of this post, you can think of the AUROC score as representing the percent liklihood that, were we to randomly select both a positive observation (modification) and negative observation (non-modification) from our dataset, the model would assign a higher predicted probability to the positive observation. In other words, the AUROC score is a useful way to measure our model's classification capability. \n",
    "\n",
    "Given our model's AUROC score is only 51%, we can conclude that our current model is virtually no better at classifying than a coin flip. Fortunately, there are easy and intuitive ways to enhance our model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsampling & Downsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A great way to correct for the impacts of an imbalanced dataset is to use common resampling methods like upsampling or downsampling. The general intuition of upsampling and downsampling is straightforward: models trained on datasets with equally represented categories will prove better classifiers. In the case of upsampling, equal representation is achieved by increasing the number of observations in the minority category to match the number of observations in the majority category. In the case of downsampling on the other hand, equal representation is achieved by decreasing the number of observations in the majority category to match the number of observations in the minority category. Let's use scikit-learn's resampling package to test these methods and their effectiveness. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    74579\n",
       "0    74579\n",
       "Name: Yes, dtype: int64"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##################################\n",
    "# Step 5) Upsample Modifications #\n",
    "##################################\n",
    "from sklearn.utils import resample\n",
    "\n",
    "#combine training sets \n",
    "comb_train_up = x_train.join(y_train)\n",
    "\n",
    "#separate by modifications & no modifications\n",
    "comb_train_up_mod = comb_train_up[comb_train_up.Yes==1]\n",
    "comb_train_up_no_mod = comb_train_up[comb_train_up.Yes==0]\n",
    "\n",
    "#make size of mods equal to size of no mods\n",
    "comb_train_up_mod = resample(comb_train_up_mod, \n",
    "                                    replace=True,\n",
    "                                    n_samples=comb_train_up_no_mod.Yes.count())\n",
    "\n",
    "#concatenate into one dataframe\n",
    "comb_train_up = pd.concat([comb_train_up_no_mod, comb_train_up_mod])\n",
    "\n",
    "#check that value_counts are equivalent\n",
    "comb_train_up.Yes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, after upsampling, our training dataset now has an even distribution of both modifications and non-modifications (where \"1\" represesnts a modification and \"0\" represents a non-modification). To demonstrate how this helps improve classification power, let's re-train our KNN model on the upsampled dataset and calculate a new AUROC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create new split and then retrain knn model on upsampled set\n",
    "x_train_up = comb_train_up.drop(\"Yes\", axis=1)\n",
    "y_train_up = comb_train_up[\"Yes\"]\n",
    "\n",
    "#refit our knn model\n",
    "knn.fit(x_train_up, y_train_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57.88\n"
     ]
    }
   ],
   "source": [
    "#use our newly trained model to predict classifications\n",
    "y_predict_knn_up = knn.predict(x_test)\n",
    "print(np.round(roc_auc_score(y_test,y_predict_knn_up)*100,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our new AUROC score reveals that, by simply upsampling our modification data, we can raise our model's classification power by about 7% -- an impressive increase gained through just a few lines of code. Let's try downsampling to see if we can do even better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3643\n",
       "0    3643\n",
       "Name: Yes, dtype: int64"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####################################### \n",
    "# 6) Downsample our Non-Modifications #\n",
    "#######################################\n",
    "#combine training sets \n",
    "comb_train_down = x_train.join(y_train)\n",
    "\n",
    "#separate by modifications & no modifications\n",
    "comb_train_down_mod = comb_train_down[comb_train_down.Yes==1]\n",
    "comb_train_down_no_mod = comb_train_down[comb_train_down.Yes==0]\n",
    "\n",
    "#make size of mods equal to size of no mods\n",
    "comb_train_down_no_mod = resample(comb_train_down_no_mod, \n",
    "                                    replace=False,\n",
    "                                    n_samples=comb_train_down_mod.Yes.count())\n",
    "\n",
    "#concatenate into one dataframe\n",
    "comb_train_down = pd.concat([comb_train_down_no_mod, comb_train_down_mod])\n",
    "\n",
    "#check that value_counts are equivalent\n",
    "comb_train_down.Yes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_down = comb_train_down.drop(\"Yes\", axis=1)\n",
    "y_train_down = comb_train_down[\"Yes\"]\n",
    "knn.fit(x_train_down, y_train_down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60.46\n"
     ]
    }
   ],
   "source": [
    "y_predict_knn_down = knn.predict(x_test)\n",
    "print(np.round(roc_auc_score(y_test,y_predict_knn_down)*100,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, downsampling does yield a better AUROC score, increasing our KNN model's classification power by an additional 2%, representing a full 9% increase over our original model's performance. \n",
    "\n",
    "While an AUROC score of 60% is still fairly low, this example is simply meant to demonstrate the power and ease of use of resampling. \n",
    "\n",
    "If we need improved performance, there is much we can do to train better models with our downsampled data. A full discussion of model optimization falls outside the scope of this post, but in the next section, I will breifly touch on some common techniques used to improve classification model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving Classification Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing we can do to further improve our model's performance is test different classification methods. Thus far, we have only used a KNN model, but there are many other popular algorithms readily available to analysts seeking to build classifiers. For this analysis, let's try using our downsampled data to train and test a Random Forest Classifier (RFC), a model which creates multiple decision trees by randomly holding out features at each iteration and then averages the resulting scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#########################################\n",
    "# Step 7) Try a RandomForestClassifier  #\n",
    "#########################################\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(x_train_down, y_train_down)\n",
    "y_predict_rfc_down = rfc.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.9\n"
     ]
    }
   ],
   "source": [
    "print(np.round(roc_auc_score(y_test,y_predict_rfc_down)*100,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! Simplpy changing our classification algorithm increased our AUROC score by nearly 11%. In just a few short steps, we have gone from a model that couldn't outperform random guessing to a model that is beginning to classify observations reasonably well.\n",
    "\n",
    "This is impressive, but if you're a competitive analyst like me, you're probably wondering, \"Can we do even better?\" The answer -- a resounding \"Yes\". \n",
    "\n",
    "The truth is, these techniques only scratch the surface of model optimization. Analysts can take their models in a multitude of directions to improve performance by testing a variety of different feature sets, classifiers, and hyperparameters. While this can prove to be a long and arduous process, it can also be insightful and, I daresay, fun. Additionally, there are many easy-to-use packages designed to expedite this process. In the next section of this post, we will examine an introductory example of how to use grid search cross validation to speed up the process of hyperparameter tuning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Grid Search Cross Validation to Tune Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search cross validation is a tool provided by scikit-learn which allows analysts to both quickly test a wide range of hyperparameters and efficiently determine which model, of the ones tested, produces the best results according to a specified scoring criteria. Let's use grid search cross validation below to see if we can improve the performance of our RFC model. Note, the grid search function tests every combination of hyperparameters the user passes it. As a consequence, the time it takes a grid search to execute increases for every additional hyperparameter the analyst includes. If you are building a model and need to optimize it in a short timeframe, consider using RandomizedSearchCV instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=25, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=5, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=1,\n",
      "            oob_score=True, random_state=None, verbose=0, warm_start=False)\n",
      "74.62\n"
     ]
    }
   ],
   "source": [
    "######################################\n",
    "# Step 8) Tune RFC with GridSearchCV #\n",
    "######################################\n",
    "\n",
    "#set nearest neighbors classifier\n",
    "rfc = RandomForestClassifier()\n",
    "estimators = [250, 500, 750, 1000, 1250]\n",
    "features = [5, \"sqrt\", 25, None]\n",
    "samples = [1, 5, 10, 25, 50]\n",
    "params = {\"n_estimators\": estimators, \"oob_score\": [True], \"max_features\": features, \"min_samples_leaf\": samples}\n",
    "\n",
    "#create grid to search\n",
    "rfc_grid = GridSearchCV(rfc, param_grid=params, scoring=\"roc_auc\", n_jobs=-1)\n",
    "rfc_grid.fit(x_train_down, y_train_down)\n",
    "#print best estimator\n",
    "print(rfc_grid.best_estimator_)\n",
    "y_predict_rfc_down_grid = rfc_grid.predict(x_test)\n",
    "\n",
    "print(np.round(roc_auc_score(y_test, y_predict_rfc_down_grid)*100,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, using grid search cross validation has increased our AUROC score by an additional 4%.  To further enhance performance, analysts could continue to use grid search, testing a variety of classifiers, features, and parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the day, there is no single perfect method analysts can use to always produce the best optimized classification model. That said, when dealing with imbalanced datasets, analysts can achieve superior classification performance through the use of easily implementable resampling methods. Additionally, to further improve performance, analysts can train various types of classification models on resampled datasets, seeking to iteratively improve AUROC (or other) scores. While at first glance imbalanced datasets might seem daunting, dealing with them can be simple, enjoyable, and can ultimately produce models which classify quite well. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
